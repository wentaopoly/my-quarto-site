import json
from pathlib import Path

def prepare_visualization_data(input_file="alice_chunks_with_kg.json", output_dir="data"):
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # è¯»å–åŸå§‹æ•°æ®
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
        print("è¯·ç¡®ä¿alice_chunks_with_kg.jsonæ–‡ä»¶åœ¨å½“å‰ç›®å½•")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯ï¼šJSONæ–‡ä»¶æ ¼å¼æœ‰è¯¯: {e}")
        return
    
    print("ğŸ” åˆ†ææ•°æ®ç»“æ„...")
    
    # è·å–å…ƒæ•°æ®
    metadata = data.get('metadata', {})
    
    # è·å–chunksçš„æ±‡æ€»å’Œè¯¦ç»†ä¿¡æ¯
    chunks_summary = data.get('chunks_summary', [])
    chunks_detail = data.get('chunks_detail', [])
    
    # åˆ›å»ºä¸€ä¸ªæ˜ å°„ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    summary_map = {chunk['chunk_id']: chunk for chunk in chunks_summary}
    
    # å¤„ç†æ¯ä¸ªchunk
    chunks_data = []
    total_entities = 0
    total_relationships = 0
    total_events = 0
    
    for i, chunk_detail in enumerate(chunks_detail):
        chunk_id = chunk_detail.get('chunk_id', i)
        
        # è·å–å¯¹åº”çš„summaryä¿¡æ¯
        chunk_summary = summary_map.get(chunk_id, {})
        
        # æå–åŸºæœ¬ä¿¡æ¯
        chunk_text = chunk_detail.get('chunk_text', '')
        chunk_metadata = chunk_detail.get('chunk_metadata', {})
        text_length = chunk_metadata.get('text_length', len(chunk_text))
        
        # è·å–çŸ¥è¯†å›¾è°±
        knowledge_graph = chunk_detail.get('knowledge_graph', {})
        
        # æå–å„ç§å®ä½“å’Œå…³ç³»
        entities = knowledge_graph.get('entities', [])
        relationships = knowledge_graph.get('relationships', [])
        key_events = knowledge_graph.get('key_events', [])
        
        # ä»å®ä½“ä¸­åˆ†ç±»å‡ºåœ°ç‚¹å’Œè§’è‰²
        locations = []
        characters = []
        objects = []
        
        for entity in entities:
            entity_type = entity.get('type', '').upper()
            if entity_type == 'LOCATION':
                locations.append(entity)
            elif entity_type in ['PERSON', 'CHARACTER', 'CREATURE']:
                characters.append(entity)
            elif entity_type == 'OBJECT':
                objects.append(entity)
        
        # ä½¿ç”¨summaryä¸­çš„ç»Ÿè®¡æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–è®¡ç®—å®é™…æ•°é‡
        num_entities = chunk_summary.get('num_entities', len(entities))
        num_relationships = chunk_summary.get('num_relationships', len(relationships))
        num_events = chunk_summary.get('num_events', len(key_events))
        
        chunk_info = {
            'id': chunk_id,
            'text': chunk_text,
            'text_preview': chunk_text[:500] + '...' if len(chunk_text) > 500 else chunk_text,
            'text_length': text_length,
            'chapter': chunk_metadata.get('chapter', 'Unknown'),
            'kg': {
                'entities': entities,
                'relationships': relationships,
                'locations': locations,
                'characters': characters,
                'objects': objects,
                'key_events': key_events,
                'stats': {
                    'num_entities': num_entities,
                    'num_relationships': num_relationships,
                    'num_locations': len(locations),
                    'num_characters': len(characters),
                    'num_objects': len(objects),
                    'num_events': num_events
                }
            },
            'metadata': {
                'has_error': chunk_summary.get('has_error', False),
                'generation_time': chunk_summary.get('generation_time', 0),
                'chapter': chunk_metadata.get('chapter', 'Unknown'),
                'start_position': chunk_metadata.get('start_position', 0),
                'end_position': chunk_metadata.get('end_position', 0)
            }
        }
        
        chunks_data.append(chunk_info)
        
        # æ›´æ–°æ€»è®¡
        total_entities += num_entities
        total_relationships += num_relationships
        total_events += num_events
        
        # æ‰“å°å‰å‡ ä¸ªchunkçš„è¯¦ç»†ä¿¡æ¯
        if i < 3:
            print(f"\nğŸ“Š Chunk {chunk_id} çš„ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - ç« èŠ‚: {chunk_info['metadata']['chapter']}")
            print(f"   - æ–‡æœ¬é•¿åº¦: {text_length}")
            print(f"   - å®ä½“æ•°é‡: {num_entities}")
            if entities:
                entity_types = set(e.get('type', 'UNKNOWN') for e in entities)
                print(f"     â€¢ å®ä½“ç±»å‹: {', '.join(entity_types)}")
                print(f"     â€¢ åœ°ç‚¹: {len(locations)} ä¸ª")
                print(f"     â€¢ è§’è‰²: {len(characters)} ä¸ª")
                print(f"     â€¢ ç‰©å“: {len(objects)} ä¸ª")
            print(f"   - å…³ç³»æ•°é‡: {num_relationships}")
            print(f"   - äº‹ä»¶æ•°é‡: {num_events}")
            
            # æ˜¾ç¤ºä¸€äº›å®ä½“ç¤ºä¾‹
            if entities and i == 0:
                print(f"\n   å®ä½“ç¤ºä¾‹:")
                for j, entity in enumerate(entities[:3]):
                    print(f"     {j+1}. {entity.get('name', 'Unknown')} ({entity.get('type', 'Unknown')})")
                    if entity.get('description'):
                        desc = entity['description'][:100] + '...' if len(entity.get('description', '')) > 100 else entity['description']
                        print(f"        {desc}")
    
    # åˆ›å»ºæ±‡æ€»æ•°æ®
    summary_data = {
        'metadata': {
            **metadata,
            'total_chunks': len(chunks_data),
            'total_entities': total_entities,
            'total_relationships': total_relationships,
            'total_events': total_events,
            'source_file': input_file
        },
        'chunks': chunks_data
    }
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    all_chunks_file = output_path / "all_chunks.json"
    with open(all_chunks_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜ç®€åŒ–çš„ç»Ÿè®¡æ•°æ®
    stats_file = output_path / "chunks_stats.json"
    stats_data = []
    for chunk in chunks_data:
        stats_data.append({
            'id': chunk['id'],
            'chapter': chunk.get('chapter', chunk['metadata'].get('chapter', 'Unknown')),
            'text_length': chunk['text_length'],
            'stats': chunk['kg']['stats']
        })
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å®ä½“æ±‡æ€»
    entities_file = output_path / "entities_summary.json"
    all_entities = []
    all_locations = []
    all_characters = []
    
    for chunk in chunks_data:
        for entity in chunk['kg']['entities']:
            entity_with_chunk = {**entity, 'chunk_id': chunk['id']}
            all_entities.append(entity_with_chunk)
            
            if entity.get('type') == 'LOCATION':
                all_locations.append(entity_with_chunk)
            elif entity.get('type') in ['PERSON', 'CHARACTER', 'CREATURE']:
                all_characters.append(entity_with_chunk)
    
    entities_summary = {
        'total_entities': len(all_entities),
        'total_unique_locations': len(set(e['name'] for e in all_locations if 'name' in e)),
        'total_unique_characters': len(set(e['name'] for e in all_characters if 'name' in e)),
        'entities': all_entities[:100],  # åªä¿å­˜å‰100ä¸ªä½œä¸ºç¤ºä¾‹
        'main_locations': list({e['name']: e for e in all_locations if 'name' in e}.values())[:20],
        'main_characters': list({e['name']: e for e in all_characters if 'name' in e}.values())[:20]
    }
    
    with open(entities_file, 'w', encoding='utf-8') as f:
        json.dump(entities_summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print(f"   - å¤„ç†äº† {len(chunks_data)} ä¸ªchunks")
    print(f"   - æ€»å®ä½“æ•°: {total_entities}")
    print(f"   - æ€»å…³ç³»æ•°: {total_relationships}")
    print(f"   - æ€»äº‹ä»¶æ•°: {total_events}")
    print(f"   - æ•°æ®ä¿å­˜åœ¨ {output_dir}/ ç›®å½•:")
    print(f"     â€¢ å®Œæ•´æ•°æ®: all_chunks.json")
    print(f"     â€¢ ç»Ÿè®¡æ•°æ®: chunks_stats.json")
    print(f"     â€¢ å®ä½“æ±‡æ€»: entities_summary.json")

def inspect_data_structure(input_file="alice_chunks_with_kg.json"):
    """æ·±å…¥æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„å®é™…ç»“æ„"""
    print("ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶ç»“æ„...")
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {e}")
        return
    
    # æ£€æŸ¥é¡¶å±‚ç»“æ„
    if isinstance(data, dict):
        print(f"âœ“ æ•°æ®æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«é”®: {list(data.keys())}")
        
        # æ£€æŸ¥metadata
        if 'metadata' in data:
            print(f"\nğŸ“‹ Metadata:")
            for key, value in data['metadata'].items():
                if not isinstance(value, (list, dict)):
                    print(f"   - {key}: {value}")
        
        # æ£€æŸ¥chunks_summary
        if 'chunks_summary' in data and data['chunks_summary']:
            print(f"\nğŸ“Š Chunks Summary: {len(data['chunks_summary'])} ä¸ªchunks")
            print(f"   ç¬¬ä¸€ä¸ªchunkçš„å­—æ®µ: {list(data['chunks_summary'][0].keys())}")
        
        # æ£€æŸ¥chunks_detail
        if 'chunks_detail' in data and data['chunks_detail']:
            print(f"\nğŸ“š Chunks Detail: {len(data['chunks_detail'])} ä¸ªchunks")
            first_chunk = data['chunks_detail'][0]
            print(f"   ç¬¬ä¸€ä¸ªchunkçš„å­—æ®µ: {list(first_chunk.keys())}")
            
            # æ£€æŸ¥knowledge_graphç»“æ„
            if 'knowledge_graph' in first_chunk:
                kg = first_chunk['knowledge_graph']
                print(f"\n   Knowledge Graph å­—æ®µ:")
                for key in kg.keys():
                    if isinstance(kg[key], list):
                        print(f"     - {key}: åˆ—è¡¨ï¼ŒåŒ…å« {len(kg[key])} ä¸ªå…ƒç´ ")
                        if kg[key] and isinstance(kg[key][0], dict):
                            print(f"       ç¬¬ä¸€ä¸ªå…ƒç´ çš„å­—æ®µ: {list(kg[key][0].keys())}")
                    else:
                        print(f"     - {key}: {type(kg[key]).__name__}")

if __name__ == "__main__":
    # å…ˆæ·±å…¥æ£€æŸ¥æ•°æ®ç»“æ„
    inspect_data_structure()
    print("\n" + "="*60 + "\n")
    # ç„¶åå‡†å¤‡å¯è§†åŒ–æ•°æ®
    prepare_visualization_data()